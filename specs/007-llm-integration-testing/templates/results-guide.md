# Results Guide

This guide explains how to store and organize test results for the LLM-Based Integration Testing Framework.

## Directory Structure

Test results are stored in a date-based hierarchy:

```
results/
├── 2025-12-08/                        # Date of test run
│   ├── report.md                      # Daily summary report
│   ├── TC-MOUSE-001/                  # Individual test result
│   │   ├── step-1-before.png          # Before composite screenshot
│   │   ├── step-1-before-meta.json    # Composite metadata (monitor regions)
│   │   ├── step-1-after.png           # After composite screenshot
│   │   ├── step-1-after-meta.json     # Composite metadata
│   │   ├── step-1-diff.png            # Visual diff image (optional)
│   │   └── result.md                  # Result document
│   ├── TC-WORKFLOW-001/               # Multi-step workflow test
│   │   ├── step-1-before.png
│   │   ├── step-1-before-meta.json
│   │   ├── step-1-after.png
│   │   ├── step-1-after-meta.json
│   │   ├── step-1-diff.png
│   │   ├── step-2-before.png          # Additional step screenshots
│   │   ├── step-2-before-meta.json
│   │   ├── step-2-after.png
│   │   ├── step-2-after-meta.json
│   │   ├── step-2-diff.png
│   │   └── result.md
│   └── TC-KEYBOARD-001/
│       ├── step-1-before.png
│       ├── step-1-before-meta.json
│       ├── step-1-after.png
│       ├── step-1-after-meta.json
│       └── result.md
└── 2025-12-09/                        # Next day's results
    └── ...
```

## File Naming Conventions

### Result Directories

- **Date Directory**: `YYYY-MM-DD` (e.g., `2025-12-08`)
- **Test Directory**: Same as test case ID (e.g., `TC-MOUSE-001`)

### Screenshot Files

#### Composite Screenshots (Recommended)

Use the step-based naming pattern for all-monitors composite screenshots:

| Filename | Purpose |
|----------|---------|
| `step-{N}-before.png` | Composite screenshot before step N action |
| `step-{N}-before-meta.json` | Metadata with monitor regions for before screenshot |
| `step-{N}-after.png` | Composite screenshot after step N action |
| `step-{N}-after-meta.json` | Metadata with monitor regions for after screenshot |
| `step-{N}-diff.png` | Visual diff image highlighting changed pixels |

**Examples**:
- `step-1-before.png`, `step-1-before-meta.json` - First step, before action
- `step-1-after.png`, `step-1-after-meta.json` - First step, after action
- `step-1-diff.png` - Visual diff between before and after
- `step-2-before.png` - Second step, before action (for multi-step workflows)

#### Metadata JSON Structure

The metadata JSON file contains composite screenshot information for region identification:

```json
{
  "VirtualScreenBounds": {
    "X": -1920, "Y": 0, "Width": 3840, "Height": 1080
  },
  "MonitorRegions": [
    {
      "Index": 0,
      "IsPrimary": true,
      "DeviceName": "\\\\.\\DISPLAY1",
      "Bounds": { "X": 0, "Y": 0, "Width": 1920, "Height": 1080 },
      "ImageX": 1920,
      "ImageY": 0
    },
    {
      "Index": 1,
      "IsPrimary": false,
      "DeviceName": "\\\\.\\DISPLAY2",
      "Bounds": { "X": -1920, "Y": 0, "Width": 1920, "Height": 1080 },
      "ImageX": 0,
      "ImageY": 0
    }
  ]
}
```

#### Legacy Screenshot Names (Deprecated)

For backward compatibility, these names are still supported but discouraged:

| Filename | Purpose |
|----------|---------|
| `before.png` | Initial state before main action |
| `after.png` | Final state after main action |
| `step-{NN}.png` | Intermediate screenshot at step number NN |

### Result Files

- `result.md` - Primary result document (required)
- `report.md` - Daily aggregate report (one per date directory)

## Creating Results

### After Running a Test

1. Create date directory if not exists: `results/YYYY-MM-DD/`
2. Create test directory: `results/YYYY-MM-DD/{TC-ID}/`
3. Save screenshots with proper names
4. Create result.md from template

### Screenshot Requirements

Per data-model.md, at least one screenshot is required per test result.

**Before Screenshot** (required):
- Capture BEFORE the main action using `target="all_monitors"`
- Save composite metadata for region identification
- Shows initial state for comparison across all monitors

**After Screenshot** (required):
- Capture AFTER the main action using `target="all_monitors"`
- Save composite metadata for region identification  
- Shows final state for verification

**Visual Diff Image** (recommended):
- Generated by comparing before and after screenshots
- Highlights changed pixels with semi-transparent red overlay
- Useful for identifying exactly what changed

**Intermediate Screenshots** (for multi-step workflows):
- Capture at significant workflow steps
- Use step-based naming: `step-2-before.png`, `step-2-after.png`
- Each step includes its own metadata JSON

### Composite Metadata Usage

The metadata JSON enables:
1. **Monitor identification**: Know which regions of the composite image correspond to which monitor
2. **Targeted verification**: Extract and analyze specific monitor regions
3. **Cross-monitor detection**: Identify when changes span multiple monitors
4. **Visual diff analysis**: Compare regions independently per monitor

### Result Document

Copy `templates/result-template.md` and fill in:

1. **Summary**: Times, duration, monitor info
2. **Pass Criteria Results**: Mark each criterion PASS or FAIL
3. **Screenshots**: Reference all screenshots with descriptions
4. **Tool Invocations**: Document each MCP tool call
5. **Observations**: LLM's narrative of what happened
6. **Failure Details**: Required if status is FAIL
7. **Environment**: System information

## Status Values

| Status | Meaning |
|--------|---------|
| **PASS** | All pass criteria verified |
| **FAIL** | One or more criteria not met |
| **BLOCKED** | Preconditions not satisfied |
| **SKIPPED** | Intentionally not executed |

### Status Rules (from data-model.md)

- Status cannot be PASS if any pass criteria result is FAIL
- BLOCKED should include which precondition failed
- SKIPPED should include reason (e.g., dependency failed)

## Daily Reports

After running tests, create a daily summary report:

1. Use `templates/report-template.md`
2. Aggregate all test results for the day
3. Calculate pass/fail/blocked/skipped counts
4. List failed tests with details
5. Include environment information

## Git Integration

By default, results are .gitignored:
- See `.gitignore` entry: `specs/007-llm-integration-testing/results/`

To track results:
1. Remove or modify the .gitignore entry
2. Commit result files

## Example Result

See `results/example/TC-EXAMPLE-001/result.md` for a complete filled-in example.

## Best Practices

### DO:
- ✅ Use `target="all_monitors"` for composite screenshots of all displays
- ✅ Save metadata JSON alongside each composite screenshot
- ✅ Use step-based naming: `step-1-before.png`, `step-1-after.png`
- ✅ Generate visual diff images for visual verification tests
- ✅ Include cursor in screenshots (`includeCursor=true`)
- ✅ Document observations even for passing tests
- ✅ Record exact tool parameters used
- ✅ Note any anomalies or warnings

### DON'T:
- ❌ Skip screenshots (at least 1 required)
- ❌ Forget to save metadata JSON for composite screenshots
- ❌ Leave template placeholders unfilled
- ❌ Mark PASS if any criterion fails
- ❌ Forget to document environment info

## Querying Results

To find results for a specific test:
```
results/*/TC-MOUSE-001/result.md
```

To find all failures:
```
grep -r "Status.*FAIL" results/*/result.md
```

To list all tests run on a date:
```
ls results/2025-12-08/*/result.md
```
